{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "051b6b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb337b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(size=(5000,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b16a25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = (torch.sin(X) / X) - (X*0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a57ab4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.flatten(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0bd9ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23c18813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d59c7c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNumberDataset(Dataset):\n",
    "    def __init__(self, X_data, y_data, preprocessing=None):\n",
    "        # Подаем наш подготовленный датафрейм\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "    def __getitem__(self, index):     \n",
    "        return (self.X_data[index], self.y_data[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c0d09a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 1 # размер картинки которую мы будет подавать в нейронные сети\n",
    "channels = 1 # количество каналов в нашей картинке\n",
    "batch_size = 16 # размер батча\n",
    "img_shape = (channels, img_size, img_size) # полный шейп нашей картинки\n",
    "\n",
    "real_data = MyNumberDataset(X,y)\n",
    "real_data = torch.utils.data.DataLoader(real_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6662c81",
   "metadata": {},
   "source": [
    "#### Генератор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c49c7df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Использование таких словарей позволяет нам варьировать параметры нашей сети в процессе использования\n",
    "        self.activations = nn.ModuleDict([\n",
    "                ['lrelu', nn.LeakyReLU(0.2, inplace=True)],\n",
    "                ['relu', nn.ReLU(0.2)]])\n",
    "        \n",
    "        def block(in_feat, out_feat, normalize=True, activation='relu'): #activation='relu\n",
    "            layers = [nn.Linear(in_feat, out_feat)] # Если мы создаем последовательность слоев - то мы задаем их\n",
    "                                                    # с помощью списка.\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(self.activations[activation]) # Эта сторчка означает тоже самое что и\n",
    "                                                       # layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(latent_dim, 12, normalize=False), # Звездочка означает unpacking списка\n",
    "            *block(12, 25, activation='lrelu'),\n",
    "            *block(25, 51),\n",
    "            *block(51, 102),\n",
    "            nn.Linear(102, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        # 2\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), *img_shape)\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5192ad3",
   "metadata": {},
   "source": [
    "#### ... Небольшое отступление \n",
    "Создание своих слоев. Если все правильно оформить, то созданный кастомный слой можно будет использовать внутри pytorch как обычный слой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6ef229e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLinearLayer(nn.Module):\n",
    "    def __init__(self, size_in, size_out):\n",
    "        super().__init__()\n",
    "        self.size_in, self.size_out = size_in, size_out\n",
    "        \n",
    "        weights = torch.Tensor(size_out, size_in)\n",
    "        self.weights = nn.Parameter(weights) # Обьявляем веса кка параметры слоя\n",
    "\n",
    "        bias = torch.Tensor(size_out)\n",
    "        self.bias = nn.Parameter(bias)\n",
    "\n",
    "        nn.init.uniform_(self.weights, -0.005, 0.005) \n",
    "        nn.init.uniform_(self.bias, -0.005, 0.005)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # По формуле линейного слоя, нам нужно умножить наши данные на трнспонированные веса и добавить смещение\n",
    "        w_times_x = torch.mm(x, self.weights.t())\n",
    "        return torch.add(w_times_x, self.bias)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff5028b",
   "metadata": {},
   "source": [
    "#### Дискриминатор:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d6c05bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            CustomLinearLayer(int(np.prod(img_shape)), 51),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            CustomLinearLayer(51, 25),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            CustomLinearLayer(25, 1),\n",
    "            \n",
    "        )\n",
    "  \n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        validity = self.model(img_flat)\n",
    "\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053b3e68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3f9593ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 200 # количество эпох\n",
    "lr = 0.0002 # шаг обучения\n",
    "\n",
    "b1 = 0.5 # гиперпараметр для оптимайзера Adam\n",
    "b2 = 0.999 # гиперпараметр для оптимайзера Adam\n",
    "\n",
    "latent_dim = 10# Размерность случайного вектора, который подается на вход генератору\n",
    "\n",
    "\n",
    "sample_interval = 25 # количество картинок для отображения процесса обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b84dc268",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# Для каждой нейронки свой опитимизатор\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "# Но вот функция ошибки у нас будет одна общая\n",
    "adversarial_loss = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c5a1c153",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_loss_history = []\n",
    "g_loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d47e0250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor(batch_size, 1).fill_(1.0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fa30000e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Intelit\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16, 1])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (16) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-586ac901986d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;31m# Тут сравниваем предсказанные значения Дискриминатора(на основе настоящих данных) с настоящими\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0md_real_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madversarial_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m# Опять делаем предсказание на Дискриминаторе с помощью сгенерированных данных\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 536\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3289\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3291\u001b[1;33m     \u001b[0mexpanded_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3292\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[attr-defined]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (16) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "red_patch = mpatches.Patch(color='red', label='D loss')\n",
    "green_patch = mpatches.Patch(color='green', label='G loss')\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (imgs, labels) in enumerate(real_data):\n",
    "        \n",
    "##################### Лейблы для данных: 1 - настоящие, 0 - сгенерированные ########\n",
    "        valid = torch.FloatTensor(batch_size, 1).fill_(1.0).to(device)\n",
    "        fake = torch.FloatTensor(batch_size, 1).fill_(0.0).to(device)\n",
    "\n",
    "        real_imgs = imgs.type(torch.FloatTensor).to(device)\n",
    "\n",
    "\n",
    "######################  Тренировка генератора    ##########################\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "    \n",
    "        #генерация шума\n",
    "        z = torch.FloatTensor(np.random.normal(0, 1, (batch_size, latent_dim))).to(device)\n",
    "        \n",
    "        # Генерируем даные Генератором на основе шума\n",
    "        gen_imgs = generator(z)\n",
    "        \n",
    "        # Подаем сгенерированые данные на Дискриминатор \n",
    "        validity = discriminator(gen_imgs)\n",
    "        \n",
    "        # Тут сравниваем предсказанные значения Дискриминатора(на основе сгенерировнных данных) с настоящими\n",
    "        g_loss = adversarial_loss(validity, valid)\n",
    "        \n",
    "        # Делаем шаг обучения нашего Генератора\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "######################  Тренировка дискриминатора    ##########################\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        # Получаем предсказания дискриминатора на основе реальных данных\n",
    "        real_pred = discriminator(real_imgs)\n",
    "        \n",
    "        # Тут сравниваем предсказанные значения Дискриминатора(на основе настоящих данных) с настоящими\n",
    "        d_real_loss = adversarial_loss(real_pred, valid)\n",
    "        \n",
    "        # Опять делаем предсказание на Дискриминаторе с помощью сгенерированных данных\n",
    "        fake_pred = discriminator(gen_imgs.detach())\n",
    "        \n",
    "        # расчитываем ошибку предсказанного с фейковыми лейблами\n",
    "        d_fake_loss = adversarial_loss(fake_pred, fake)\n",
    "        \n",
    "        # И усредняем два лосса в один\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "######## Отображение процесса обучения и вывод функций потерь ############\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dd4563",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
